"""
å®Ÿéš›ã®ã‚°ãƒ©ãƒ•å®Ÿè¡Œçµæœã§Provenanceæ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€å…ˆã»ã©å®Ÿè¡Œã—ãŸç ”ç©¶ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ã€
Provenanceæ©Ÿèƒ½ã‚’å®Ÿéš›ã«ä½¿ç”¨ã—ã¦ã€ã‚½ãƒ¼ã‚¹è¿½è·¡ã¨å¼•ç”¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’å®Ÿæ¼”ã—ã¾ã™ã€‚
"""

import json
from pathlib import Path
from datetime import datetime


def load_latest_report():
    """æœ€æ–°ã®ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    # Script is in examples/provenance/, reports are in ../../reports/
    reports_dir = Path("../../reports")

    # æœ€æ–°ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’æ¢ã™
    report_files = sorted(reports_dir.glob("report_*.md"), key=lambda p: p.stat().st_mtime, reverse=True)

    if not report_files:
        print("âŒ ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None

    latest = report_files[0]
    print(f"ğŸ“„ æœ€æ–°ãƒ¬ãƒãƒ¼ãƒˆ: {latest.name}")

    with open(latest) as f:
        content = f.read()

    return content


def extract_references(report_content: str) -> list:
    """ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰Referencesã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
    if "**References**" not in report_content:
        return []

    references_section = report_content.split("**References**")[1]

    # å„ã‚½ãƒ¼ã‚¹ã‚’è§£æ
    sources = []
    lines = references_section.strip().split("\n")

    current_source = {}
    for line in lines:
        line = line.strip()

        # ã‚½ãƒ¼ã‚¹ç•ªå·ï¼ˆä¾‹: 1. "Title"ï¼‰
        if line and line[0].isdigit() and ". " in line:
            if current_source:
                sources.append(current_source)

            # ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¿ã‚¤ãƒ—ã‚’æŠ½å‡º
            parts = line.split(" - Type: ")
            title = parts[0].split(". ", 1)[1].strip('"')
            source_type = parts[1] if len(parts) > 1 else "Unknown"

            current_source = {
                "number": int(line.split(".")[0]),
                "title": title,
                "type": source_type
            }

        # URLè¡Œ
        elif line.startswith("URL:"):
            current_source["url"] = line.replace("URL:", "").strip()

        # Fileè¡Œ
        elif line.startswith("File:"):
            current_source["file"] = line.replace("File:", "").strip()

        # Relevanceè¡Œ
        elif line.startswith("Relevance:"):
            relevance_str = line.replace("Relevance:", "").strip()
            current_source["relevance"] = float(relevance_str)

    if current_source:
        sources.append(current_source)

    return sources


def export_citations_real(sources: list, format: str = "bibtex") -> str:
    """å®Ÿéš›ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰å¼•ç”¨ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""

    if format == "bibtex":
        entries = []
        year = datetime.now().year

        for source in sources:
            source_num = source["number"]
            title = source.get("title", "Unknown")
            source_type = source.get("type", "Unknown")
            key = f"source{year}_{source_num}"

            if source_type == "Web" and "url" in source:
                url = source["url"]
                entry = f"""@online{{{key},
  title = {{{title}}},
  url = {{{url}}},
  year = {{{year}}},
  note = {{Accessed: {datetime.now().strftime('%Y-%m-%d')}}}
}}"""
            else:
                file_path = source.get("file", "Unknown")
                entry = f"""@techreport{{{key},
  title = {{{title}}},
  institution = {{Internal Knowledge Base}},
  year = {{{year}}},
  note = {{Source: {file_path}}}
}}"""

            entries.append(entry)

        return "\n\n".join(entries)

    elif format == "apa":
        citations = []
        year = datetime.now().year

        for source in sources:
            source_num = source["number"]
            title = source.get("title", "Unknown")
            source_type = source.get("type", "Unknown")

            if source_type == "Web" and "url" in source:
                url = source["url"]
                citation = f"{source_num}. {title}. ({year}). Retrieved from {url}"
            else:
                file_path = source.get("file", "Unknown")
                citation = f"{source_num}. {title}. ({year}). Internal Knowledge Base. Source: {file_path}"

            citations.append(citation)

        return "\n\n".join(citations)

    elif format == "mla":
        citations = []
        access_date = datetime.now().strftime("%d %b. %Y")

        for source in sources:
            source_num = source["number"]
            title = source.get("title", "Unknown")
            source_type = source.get("type", "Unknown")

            if source_type == "Web" and "url" in source:
                url = source["url"]
                citation = f'{source_num}. "{title}." Web. {access_date}. <{url}>.'
            else:
                file_path = source.get("file", "Unknown")
                citation = f'{source_num}. "{title}." Internal Knowledge Base. {access_date}. Source: {file_path}.'

            citations.append(citation)

        return "\n\n".join(citations)


def query_claim_in_report(report: str, sources: list, claim: str):
    """ãƒ¬ãƒãƒ¼ãƒˆå†…ã®ç‰¹å®šã®ä¸»å¼µã‚’æ¤œç´¢ã—ã¦ã‚½ãƒ¼ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹"""

    # ãƒ¬ãƒãƒ¼ãƒˆå†…ã§ä¸»å¼µã‚’æ¤œç´¢
    report_lower = report.lower()
    claim_lower = claim.lower()

    if claim_lower not in report_lower:
        return {
            "claim": claim,
            "found": False,
            "message": "ä¸»å¼µãŒãƒ¬ãƒãƒ¼ãƒˆå†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
        }

    # ä¸»å¼µã®å‘¨è¾ºãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    pos = report_lower.index(claim_lower)
    start = max(0, pos - 200)
    end = min(len(report), pos + len(claim) + 200)
    context = report[start:end]

    # å¼•ç”¨ç•ªå·ã‚’æ¤œç´¢ [1], [2], etc.
    import re
    citation_pattern = r'\[(\d+)\]'
    cited_sources = []

    for match in re.finditer(citation_pattern, context):
        cite_num = int(match.group(1))

        # ã‚½ãƒ¼ã‚¹ã‚’æ¢ã™
        for source in sources:
            if source["number"] == cite_num:
                cited_sources.append(source)
                break

    return {
        "claim": claim,
        "found": True,
        "context": context.strip(),
        "sources": cited_sources,
        "source_count": len(cited_sources)
    }


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""

    print("=" * 80)
    print("ğŸ”¬ Test-Smith Provenanceæ©Ÿèƒ½ - å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    print()

    # Step 1: æœ€æ–°ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
    print("ğŸ“‚ æœ€æ–°ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
    report_content = load_latest_report()

    if not report_content:
        return

    print("âœ… ãƒ¬ãƒãƒ¼ãƒˆèª­ã¿è¾¼ã¿å®Œäº†\n")

    # Step 2: Referencesã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è§£æ
    print("=" * 80)
    print("ğŸ“– References ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è§£æ")
    print("=" * 80)
    print()

    sources = extract_references(report_content)

    if not sources:
        print("âŒ ã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    # ã‚½ãƒ¼ã‚¹çµ±è¨ˆ
    web_sources = [s for s in sources if s.get("type") == "Web"]
    kb_sources = [s for s in sources if s.get("type") == "Knowledge Base"]

    print(f"ç·ã‚½ãƒ¼ã‚¹æ•°: {len(sources)}")
    print(f"  - Web sources: {len(web_sources)}")
    print(f"  - Knowledge Base sources: {len(kb_sources)}")
    print()

    # Top 3 sources
    print("ğŸ† Top 3 Sources:")
    for source in sources[:3]:
        print(f"  [{source['number']}] {source['title']}")
        print(f"      Type: {source['type']}")
        if "url" in source:
            print(f"      URL: {source['url']}")
        if "file" in source:
            print(f"      File: {source['file']}")
        if "relevance" in source:
            print(f"      Relevance: {source['relevance']:.2f}")
        print()

    # Step 3: ç‰¹å®šã®ä¸»å¼µã®æ ¹æ‹ ã‚’ç¢ºèª
    print("=" * 80)
    print("ğŸ” ç‰¹å®šã®ä¸»å¼µã®æ ¹æ‹ ã‚’ç¢ºèª")
    print("=" * 80)
    print()

    # ãƒ¬ãƒãƒ¼ãƒˆå†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦ä¸»å¼µã‚’æ¢ã™
    test_claims = [
        "RAG",
        "retrieval",
        "accuracy"
    ]

    for claim_word in test_claims:
        if claim_word.lower() in report_content.lower():
            # ã“ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ–‡ã‚’æŠ½å‡º
            sentences = report_content.split(".")
            for sentence in sentences:
                if claim_word.lower() in sentence.lower() and len(sentence.strip()) > 20:
                    claim = sentence.strip()[:100]  # æœ€åˆã®100æ–‡å­—
                    break
            else:
                continue

            print(f"ä¸»å¼µ: \"{claim}...\"")
            print()

            provenance = query_claim_in_report(report_content, sources, claim)

            if provenance.get("found") and provenance.get("source_count") > 0:
                print(f"âœ… ã“ã®ä¸»å¼µã‚’æ”¯æŒã™ã‚‹ã‚½ãƒ¼ã‚¹æ•°: {provenance['source_count']}")
                print()
                print("ğŸ’¡ æ”¯æŒã‚½ãƒ¼ã‚¹:")
                for source in provenance["sources"][:3]:
                    print(f"  [{source['number']}] {source['title']}")
                    print(f"      Type: {source['type']}")
                    if "relevance" in source:
                        print(f"      Relevance: {source['relevance']:.2f}")
                print()

            break  # 1ã¤ã ã‘ãƒ†ã‚¹ãƒˆ

    # Step 4: å¼•ç”¨ã‚’è¤‡æ•°ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    print("=" * 80)
    print("ğŸ“„ å¼•ç”¨ã‚’è¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    print("=" * 80)
    print()

    # æœ€åˆã®5ã‚½ãƒ¼ã‚¹ã®ã¿
    sources_subset = sources[:5]

    # BibTeX
    print("ğŸ“– BibTeXå½¢å¼ (æœ€åˆã®5ã‚¨ãƒ³ãƒˆãƒª):")
    print("-" * 80)
    bibtex = export_citations_real(sources_subset, format="bibtex")
    print(bibtex)
    print()

    # APA
    print("ğŸ“˜ APAå½¢å¼ (æœ€åˆã®5ã‚¨ãƒ³ãƒˆãƒª):")
    print("-" * 80)
    apa = export_citations_real(sources_subset, format="apa")
    print(apa)
    print()

    # MLA
    print("ğŸ“— MLAå½¢å¼ (æœ€åˆã®5ã‚¨ãƒ³ãƒˆãƒª):")
    print("-" * 80)
    mla = export_citations_real(sources_subset, format="mla")
    print(mla)
    print()

    # Step 5: ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    print("=" * 80)
    print("ğŸ’¾ å¼•ç”¨ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    print("=" * 80)
    print()

    with open("../../citations/real_citations_bibtex.bib", "w") as f:
        f.write(export_citations_real(sources, format="bibtex"))
    print("âœ… BibTeXå¼•ç”¨ã‚’ä¿å­˜: citations/real_citations_bibtex.bib")

    with open("../../citations/real_citations_apa.txt", "w") as f:
        f.write(export_citations_real(sources, format="apa"))
    print("âœ… APAå¼•ç”¨ã‚’ä¿å­˜: citations/real_citations_apa.txt")

    with open("../../citations/real_citations_mla.txt", "w") as f:
        f.write(export_citations_real(sources, format="mla"))
    print("âœ… MLAå¼•ç”¨ã‚’ä¿å­˜: citations/real_citations_mla.txt")

    # ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print("ğŸ‰ å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§ã®Provenanceæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    print("=" * 80)
    print()
    print(f"ç·ã‚½ãƒ¼ã‚¹æ•°: {len(sources)}")
    print(f"  - Web: {len(web_sources)}")
    print(f"  - Knowledge Base: {len(kb_sources)}")
    print()
    print("ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
    print("  1. citations/real_citations_bibtex.bib - BibTeXå½¢å¼ã®å¼•ç”¨")
    print("  2. citations/real_citations_apa.txt - APAå½¢å¼ã®å¼•ç”¨")
    print("  3. citations/real_citations_mla.txt - MLAå½¢å¼ã®å¼•ç”¨")
    print()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\n\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
